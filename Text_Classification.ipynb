{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HVRZkRtXJmN",
        "outputId": "41857fb6-7b73-4231-e4ca-0f023c4510f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install num2words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-Td7umQX2mN",
        "outputId": "a943c50b-1c09-4ef0-96d2-97b41892c4b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting num2words\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2 (from num2words)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=5f24b98bfe168b6c0cee304a169cc4172710bff4b5933b42ed0958bf0f9b9ae1\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "LoG4PapQZFJI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "8Q7FVp6KsST-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet, gutenberg, stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "y_vU39MFYEt9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "ivmM5nE3Yaw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72eec11d-3127-44cc-e12c-2d25b4710f48"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text and Number manipulation\n",
        "import re\n",
        "import string\n",
        "from string import punctuation\n",
        "from num2words import num2words"
      ],
      "metadata": {
        "id": "QEMFXHZWYNnr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "wordnet_map = {\n",
        "    'N': wordnet.NOUN,\n",
        "    'V': wordnet.VERB,\n",
        "    'J': wordnet.ADJ,\n",
        "    'R': wordnet.ADV\n",
        "}\n"
      ],
      "metadata": {
        "id": "JEUei1vqYRri"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/gdrive/MyDrive/train.csv')\n",
        "test = pd.read_csv('/content/gdrive/MyDrive/test.csv')"
      ],
      "metadata": {
        "id": "5O9u8GIGYXbA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine columns except Survived\n",
        "column_list = ['PassengerId', 'Pclass', 'Name', 'Sex', \"Age\", 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked', 'Cabin' ]\n",
        "train['concatenated'] = train[column_list].astype(str).agg(' '.join, axis =1)\n",
        "test['concatenated'] = train[column_list].astype(str).agg(' '.join, axis =1)"
      ],
      "metadata": {
        "id": "XDQFMHyiZL3g"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_number(text):\n",
        "    # Define a regular expression pattern to capture the letter and number separately\n",
        "    pattern = r\"(?P<letter>[A-Za-z]*)(?P<number>\\d+)\"\n",
        "\n",
        "    # Define a function to convert the matched number to words\n",
        "    def replace(match):\n",
        "        # Extract the letter and number from the match\n",
        "        letter = match.group('letter')\n",
        "        number = int(match.group('number'))\n",
        "\n",
        "        # Convert the number to words\n",
        "        words = num2words(number)\n",
        "\n",
        "        # Return the original letter with the number in words\n",
        "        return f\"{letter} {words}\" if letter else words\n",
        "\n",
        "    # Use regular expression substitution to replace the number with words\n",
        "    return re.sub(pattern, replace, text)"
      ],
      "metadata": {
        "id": "fG_OfKhyp8Sh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is able to avoid converting C85 to Ceighty five\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0R-wknloZXlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_html(text):\n",
        "    Cleaner = re.compile('<.*?:') #\n",
        "    text = re.sub(Cleaner, ' ', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]',' ', text, flags=re.I)\n",
        "    return text"
      ],
      "metadata": {
        "id": "zUEUrnHGZcf0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pattern <.?: will match any substring that starts with \"<\" and ends with \":\". The .? part ensures that the pattern matches the shortest possible substring between \"<\" and \":\".\n",
        "re.sub() function is being used to replace substrings in the text variable that match the pattern specified by CLEANR with a space ' '.\n",
        "re.sub(r'[^a-zA-Z\\s]',' ', text, flags=re.I) removes any characters from the text string that are not alphabetic characters (both uppercase and lowercase) or whitespace characters.\n",
        "flags=re.I stands for re.IGNORECASE"
      ],
      "metadata": {
        "id": "P8e12DzRZjBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "    return [word for word in word_tokenize(text) if not word in stopwords]"
      ],
      "metadata": {
        "id": "VGayotpylPpr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rem_lines(text):\n",
        "    return text.strip().replace('\\n',' ')"
      ],
      "metadata": {
        "id": "42qNZNWXZigV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_punctuation(text):\n",
        "    return ''.join(c for c in text if not c in punctuation)+ ' '"
      ],
      "metadata": {
        "id": "txmxEe5JZnrR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_words(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    pos_tagged_text = nltk.pos_tag(text.split())\n",
        "    return ' '.join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])"
      ],
      "metadata": {
        "id": "T3F6l7ObZpoH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If a POS tag is not provided, the default POS tag used is wordnet.NOUN."
      ],
      "metadata": {
        "id": "MVni2TtdZthK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['concatenated'] = train['concatenated'].astype('str').apply(to_number)\n",
        "train['concatenated'] = train['concatenated'].astype('str').apply(clean_html)\n",
        "train['concatenated'] = train['concatenated'].astype('str').apply(remove_stopwords)\n",
        "train['concatenated'] = train['concatenated'].astype('str').apply(strip_punctuation)\n",
        "train['concatenated'] = train['concatenated'].astype('str').apply(lemmatize_words)\n",
        "\n",
        "\n",
        "test['concatenated'] = test['concatenated'].astype('str').apply(to_number)\n",
        "test['concatenated'] = test['concatenated'].astype('str').apply(clean_html)\n",
        "test['concatenated'] = test['concatenated'].astype('str').apply(remove_stopwords)\n",
        "test['concatenated'] = test['concatenated'].astype('str').apply(strip_punctuation)\n",
        "test['concatenated'] = test['concatenated'].astype('str').apply(lemmatize_words)"
      ],
      "metadata": {
        "id": "PnB3LuFGZ0kN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = train.concatenated.str.cat(sep=' ')\n",
        "words = nltk.tokenize.word_tokenize(txt) #tokenizes the concatenated text string into individual words\n",
        "word_dist = nltk.FreqDist(words)#creates a frequency distribution of words\n",
        "rslt = pd.DataFrame(word_dist.most_common(10),\n",
        "                    columns=['Word', 'Frequency']).set_index('Word')\n",
        "matplotlib.style.use('ggplot')\n",
        "rslt.plot.bar(rot=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "rheySuMuseEt",
        "outputId": "31f7c9fd-fb4f-4f0b-89da-f9949a338ee1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='Word'>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(train['concatenated'],train['Survived'], test_size=.30, random_state=88)"
      ],
      "metadata": {
        "id": "B6Wn0Vm_s9_S"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bag of words"
      ],
      "metadata": {
        "id": "_3Ur1zOlWkhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score as CVScore\n",
        "cv = CountVectorizer(binary=True,ngram_range=(1,2),min_df=0.0, max_df=1.0)\n",
        "cv_train = cv.fit_transform(X_train)\n",
        "cv_test = cv.transform(X_test)"
      ],
      "metadata": {
        "id": "WhwKO8NRuEiu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF Grouping"
      ],
      "metadata": {
        "id": "2oBJ5iTBWm09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer as TFIDF\n",
        "tv = TFIDF(use_idf=True, min_df=0.0, max_df=1.0)\n",
        "tv_train = tv.fit_transform(X_train)\n",
        "tv_test = tv.transform(X_test)"
      ],
      "metadata": {
        "id": "P5RqiKkluF9P"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models\n",
        "\n",
        "I am using professor's code to train and predict. The structure of the code is neat and clean."
      ],
      "metadata": {
        "id": "e2VA_VqJW2Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report as CR\n",
        "from sklearn.naive_bayes import BernoulliNB as NB\n",
        "from sklearn.linear_model import LogisticRegression as LR\n",
        "from sklearn.svm import LinearSVC as SVM\n",
        "from sklearn.linear_model import SGDClassifier as SGD\n",
        "from sklearn.ensemble import RandomForestClassifier as RFC\n",
        "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
        "from sklearn.linear_model import Perceptron as P\n",
        "\n",
        "nb=NB(alpha=.1)\n",
        "lr=LR(penalty='l2', max_iter=500, C=1, random_state=42)\n",
        "svm=SVM(penalty='l2', max_iter=50, C=1, random_state=42)\n",
        "sgd=SGD(loss='hinge', penalty=\"l2\", max_iter=500, random_state=42)\n",
        "rfc=RFC(n_estimators=50, random_state=42)\n",
        "gbc=GBC(n_estimators=50, random_state=42)\n",
        "p=P(tol=1e-3, random_state=0)\n",
        "\n",
        "models=[nb, lr,svm, sgd, rfc, gbc, p]\n",
        "\n",
        "def fit(x1,y1,x2,y2):\n",
        "    for model in models:\n",
        "        model.fit(x1,y1)\n",
        "        print(model)\n",
        "        mypred=model.predict(x2)\n",
        "        print(CR(mypred, y2))\n",
        "\n",
        "\n",
        "fit(cv_train,y_train, cv_test, y_test)# try on bag of words grouping data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gO303vdSvLvU",
        "outputId": "8fdb3628-0051-4aad-9c69-757805715c7f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BernoulliNB(alpha=0.1)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.84       176\n",
            "           1       0.70      0.71      0.70        92\n",
            "\n",
            "    accuracy                           0.79       268\n",
            "   macro avg       0.77      0.77      0.77       268\n",
            "weighted avg       0.80      0.79      0.80       268\n",
            "\n",
            "LogisticRegression(C=1, max_iter=500, random_state=42)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.83      0.83       176\n",
            "           1       0.68      0.68      0.68        92\n",
            "\n",
            "    accuracy                           0.78       268\n",
            "   macro avg       0.76      0.76      0.76       268\n",
            "weighted avg       0.78      0.78      0.78       268\n",
            "\n",
            "LinearSVC(C=1, max_iter=50, random_state=42)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.82      0.83       177\n",
            "           1       0.67      0.68      0.67        91\n",
            "\n",
            "    accuracy                           0.78       268\n",
            "   macro avg       0.75      0.75      0.75       268\n",
            "weighted avg       0.78      0.78      0.78       268\n",
            "\n",
            "SGDClassifier(max_iter=500, random_state=42)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.82      0.83       178\n",
            "           1       0.66      0.68      0.67        90\n",
            "\n",
            "    accuracy                           0.77       268\n",
            "   macro avg       0.75      0.75      0.75       268\n",
            "weighted avg       0.77      0.77      0.77       268\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier(n_estimators=50, random_state=42)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.83      0.85       186\n",
            "           1       0.66      0.74      0.70        82\n",
            "\n",
            "    accuracy                           0.80       268\n",
            "   macro avg       0.77      0.79      0.78       268\n",
            "weighted avg       0.81      0.80      0.81       268\n",
            "\n",
            "GradientBoostingClassifier(n_estimators=50, random_state=42)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.84       177\n",
            "           1       0.69      0.70      0.70        91\n",
            "\n",
            "    accuracy                           0.79       268\n",
            "   macro avg       0.77      0.77      0.77       268\n",
            "weighted avg       0.79      0.79      0.79       268\n",
            "\n",
            "Perceptron()\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.81      0.82       181\n",
            "           1       0.62      0.67      0.64        87\n",
            "\n",
            "    accuracy                           0.76       268\n",
            "   macro avg       0.73      0.74      0.73       268\n",
            "weighted avg       0.77      0.76      0.76       268\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fit(tv_train,y_train, tv_test, y_test)#try on TF-IDF grouping data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0bBA99avl8F",
        "outputId": "b24e87bf-480a-4345-d280-2e383d60e420"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BernoulliNB(alpha=0.1)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.81      0.83       181\n",
            "           1       0.63      0.68      0.66        87\n",
            "\n",
            "    accuracy                           0.77       268\n",
            "   macro avg       0.74      0.75      0.74       268\n",
            "weighted avg       0.77      0.77      0.77       268\n",
            "\n",
            "LogisticRegression(C=1, max_iter=500, random_state=42)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.82      0.85       190\n",
            "           1       0.62      0.74      0.68        78\n",
            "\n",
            "    accuracy                           0.79       268\n",
            "   macro avg       0.75      0.78      0.76       268\n",
            "weighted avg       0.81      0.79      0.80       268\n",
            "\n",
            "LinearSVC(C=1, max_iter=50, random_state=42)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.83      0.84       181\n",
            "           1       0.67      0.71      0.69        87\n",
            "\n",
            "    accuracy                           0.79       268\n",
            "   macro avg       0.76      0.77      0.77       268\n",
            "weighted avg       0.80      0.79      0.79       268\n",
            "\n",
            "SGDClassifier(max_iter=500, random_state=42)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.81      0.79       168\n",
            "           1       0.66      0.61      0.63       100\n",
            "\n",
            "    accuracy                           0.74       268\n",
            "   macro avg       0.72      0.71      0.71       268\n",
            "weighted avg       0.73      0.74      0.73       268\n",
            "\n",
            "RandomForestClassifier(n_estimators=50, random_state=42)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.81      0.84       192\n",
            "           1       0.60      0.74      0.66        76\n",
            "\n",
            "    accuracy                           0.79       268\n",
            "   macro avg       0.74      0.77      0.75       268\n",
            "weighted avg       0.81      0.79      0.79       268\n",
            "\n",
            "GradientBoostingClassifier(n_estimators=50, random_state=42)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.82      0.84       181\n",
            "           1       0.66      0.70      0.68        87\n",
            "\n",
            "    accuracy                           0.78       268\n",
            "   macro avg       0.75      0.76      0.76       268\n",
            "weighted avg       0.79      0.78      0.79       268\n",
            "\n",
            "Perceptron()\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.81      0.81       177\n",
            "           1       0.63      0.65      0.64        91\n",
            "\n",
            "    accuracy                           0.75       268\n",
            "   macro avg       0.73      0.73      0.73       268\n",
            "weighted avg       0.76      0.75      0.75       268\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN\n",
        "I give a try using rnn to train and predict the concatenated data."
      ],
      "metadata": {
        "id": "UDkPWmib3J1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input,  Bidirectional"
      ],
      "metadata": {
        "id": "RiFZVn4axZ6v"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mybatch=64   #set batch size\n",
        "embedding_size=512 #set out size for embedding\n",
        "num_epochs=4 #set the number of epochs..\n",
        "num_words = 50000     #number of words to tokenize, 50000 was start\n",
        "tf.random.set_seed(1234) #set random number seed for tensorflow, python, etc."
      ],
      "metadata": {
        "id": "CkabnuMcxxav"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov_token = '<UNK>'   #out of vocabulary replacement\n",
        "pad_type = 'post'     #padding type\n",
        "trunc_type = 'post'   #truncation type\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
        "\n",
        "tokenizer.fit_on_texts(X_train.astype('str'))\n",
        "#tokenizer.fit_on_texts(x_train2.astype('str'))\n",
        "\n",
        "# Encode training data sentences into sequences\n",
        "train_sequences1 = tokenizer.texts_to_sequences(X_train.astype('str'))\n",
        "\n",
        "# Get max training sequence length\n",
        "maxlen1 = max([len(x) for x in train_sequences1])\n",
        "\n",
        "# Pad the training sequences to the maximum sentence length\n",
        "x_train1 = pad_sequences(train_sequences1, padding=pad_type, truncating=trunc_type, maxlen=maxlen1)"
      ],
      "metadata": {
        "id": "qcSXKkztvxdo"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode test data sentences into sequences\n",
        "test_sequences1 = tokenizer.texts_to_sequences(X_test.astype('str'))\n",
        "\n",
        "# Pad the training sequences to the maximum sentence length of training set\n",
        "x_test1 = pad_sequences(test_sequences1, padding=pad_type, truncating=trunc_type, maxlen=maxlen1)"
      ],
      "metadata": {
        "id": "lzqrdoKSxy8-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RNN\n",
        "input_1 = Input(shape=(x_train1.shape[1]))\n",
        "v=Embedding(len(tokenizer.word_index) + 1, embedding_size)(input_1)\n",
        "v=Bidirectional(LSTM(embedding_size, return_sequences=True, dropout=.5))(v)\n",
        "v=Bidirectional(LSTM(embedding_size, return_sequences=False, dropout=.2))(v)\n",
        "output=Dense(1, activation='sigmoid')(v)\n",
        "model=Model(inputs=[input_1], outputs=output)"
      ],
      "metadata": {
        "id": "SADkJ9Ykybw4"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model\n",
        "SGD= tf.keras.optimizers.SGD(lr=0.0001, nesterov=True)\n",
        "adam=tf.keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
        "          optimizer=adam, metrics=['binary_accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYnnysQpygB1",
        "outputId": "5d50903e-658f-4444-fc21-5c2e6cf2a012"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 43)]              0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, 43, 512)           633344    \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirecti  (None, 43, 1024)          4198400   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirecti  (None, 1024)              6295552   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 1025      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11128321 (42.45 MB)\n",
            "Trainable params: 11128321 (42.45 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(\n",
        "              x=x_train1,y=y_train,\n",
        "              batch_size=mybatch,verbose=1,epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr_JeBW0z1TN",
        "outputId": "457da6bf-2071-495f-bd93-c92a041042a8"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "10/10 [==============================] - 86s 8s/step - loss: 0.6619 - binary_accuracy: 0.6565\n",
            "Epoch 2/4\n",
            "10/10 [==============================] - 73s 7s/step - loss: 0.4933 - binary_accuracy: 0.8090\n",
            "Epoch 3/4\n",
            "10/10 [==============================] - 79s 8s/step - loss: 0.3549 - binary_accuracy: 0.8555\n",
            "Epoch 4/4\n",
            "10/10 [==============================] - 81s 8s/step - loss: 0.2427 - binary_accuracy: 0.9181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mypred=model.predict(x_test1)\n",
        "print(CR(np.round(np.squeeze(mypred),0), y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX82w3Gn0b0O",
        "outputId": "a4ba49bb-0937-4eea-9191-1f7a13105cf7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 12s 955ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.85      0.83       165\n",
            "         1.0       0.74      0.67      0.70       103\n",
            "\n",
            "    accuracy                           0.78       268\n",
            "   macro avg       0.77      0.76      0.77       268\n",
            "weighted avg       0.78      0.78      0.78       268\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j0iYkMUUjLpu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
