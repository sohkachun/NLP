{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Filter out the specific warning\n",
        "warnings.filterwarnings(\"ignore\", message=\"overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\")"
      ],
      "metadata": {
        "id": "OggAegEDmIMM"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import BertForQuestionAnswering, BertTokenizer\n",
        "import torch\n",
        "\n",
        "# Step 1: Web Scraping and preprocess\n",
        "def fetch_and_scrape(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Will raise an HTTPError for bad requests (4XX or 5XX)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    text = ' '.join([p.text for p in soup.find_all('p')])  # Extract text from all paragraph tags\n",
        "    return text\n",
        "\n",
        "context = fetch_and_scrape('https://www.fisheries.noaa.gov/species/giant-manta-ray')\n",
        "\n",
        "# Step 3: Load Pretrained Model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "# Step 4: Tokenize Input\n",
        "def tokenize_input(question, context):\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\", max_length=512, truncation=True, padding=True, truncation_strategy = 'only_second')\n",
        "    return inputs\n",
        "\n",
        "# Step 5: Question Answering\n",
        "# Define a function to extract the answer to a question from the BERT model's output\n",
        "def get_answer(inputs):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        answer_start_scores = outputs.start_logits\n",
        "        answer_end_scores = outputs.end_logits\n",
        "\n",
        "    answer_start = torch.argmax(answer_start_scores)\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(\n",
        "        tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end])\n",
        "    )\n",
        "    return answer\n",
        "\n",
        "\n",
        "# Step 6: Extract Answer\n",
        "def extract_answer(question, context):\n",
        "    inputs = tokenize_input(question, context)\n",
        "    answer = get_answer(inputs)\n",
        "    return answer\n",
        "\n",
        "# Step 7: Display Answers\n",
        "def display_answer(question, answer):\n",
        "    print(\"Question:\", question)\n",
        "    print(\"Answer:\", answer)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa0Aq392jA9E",
        "outputId": "923ecdbb-2f17-4241-878d-48d4b95eb05a"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'What is the manta ray ?'\n",
        "answer = extract_answer(question, context)\n",
        "display_answer(question, answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtpJy9PuyMSY",
        "outputId": "76da3234-55b7-461e-bd2f-acd1799576b9"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the manta ray ?\n",
            "Answer: world â€™ s largest ray\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'What is the manta ray population status?'\n",
        "answer = extract_answer(question, context)\n",
        "display_answer(question, answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G30MrfcLkkAQ",
        "outputId": "93c39da2-8906-49dd-8671-4c866aa2d22f"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the manta ray population status?\n",
            "Answer: unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'What is the main threat for manta rays?'\n",
        "answer = extract_answer(question, context)\n",
        "display_answer(question, answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_3puoJbkBXA",
        "outputId": "660a86f2-943a-463a-fcea-30f545aa637f"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the main threat for manta rays?\n",
            "Answer: commercial fishing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'What are the color types?'\n",
        "answer = extract_answer(question, context)\n",
        "display_answer(question, answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97KNgCkbkNu9",
        "outputId": "74f0f24d-7945-4362-9119-1d86c1a709ec"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the color types?\n",
            "Answer: chevron ( mostly black back and white belly ) and black ( almost completely black on both sides )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'What is the appearance of manta rays?'\n",
        "answer = extract_answer(question, context)\n",
        "display_answer(question, answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEReI0OYlBMx",
        "outputId": "58d52667-db71-4223-d48d-7d0f89005687"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the appearance of manta rays?\n",
            "Answer: large diamond - shaped body with elongated wing - like pectoral fins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'What is manta rays diet ?'\n",
        "answer = extract_answer(question, context)\n",
        "display_answer(question, answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjJRzx31lMOH",
        "outputId": "65b3e4c3-7b43-4ce9-a6dc-8f9bd1fcdfe8"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is manta rays diet ?\n",
            "Answer: zooplankton\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Methods to obtain longer answer"
      ],
      "metadata": {
        "id": "AwhU9FHzwp1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(inputs, answer_strategy='expanded', **kwargs):\n",
        "    if answer_strategy == 'expanded':\n",
        "        return get_expanded_answer(inputs, **kwargs)\n",
        "    elif answer_strategy == 'multiple':\n",
        "        return get_multiple_answers(inputs, **kwargs)\n",
        "    elif answer_strategy == 'with_context':\n",
        "        return get_answer_with_context(inputs, **kwargs)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid answer strategy\")"
      ],
      "metadata": {
        "id": "URP1P4f2ugDY"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_expanded_answer(inputs, expansion_tokens=50):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        answer_start_scores, answer_end_scores = outputs.start_logits, outputs.end_logits\n",
        "\n",
        "    answer_start = torch.argmax(answer_start_scores)\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "    # Expanding the context around the answer\n",
        "    start_expansion = max(answer_start - expansion_tokens, 0)  # Ensure start is not negative\n",
        "    end_expansion = min(answer_end + expansion_tokens, inputs.input_ids.size(1))  # Ensure end does not exceed input size\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs.input_ids[0][start_expansion:end_expansion]))\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "JG400a6Mokc1"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a function to extract multiple answers\n",
        "def get_multiple_answers(inputs, num_answers=3):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        answer_start_scores, answer_end_scores = outputs.start_logits, outputs.end_logits\n",
        "\n",
        "    # Get the top 'num_answers' start and end positions\n",
        "    top_starts = torch.topk(answer_start_scores, num_answers).indices\n",
        "    top_ends = torch.topk(answer_end_scores, num_answers).indices\n",
        "\n",
        "    answers = []\n",
        "    for i, (start, end) in enumerate(zip(top_starts[0], top_ends[0])):\n",
        "        if end >= start:  # Ensure valid index ordering\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs.input_ids[0][start:end+1]))\n",
        "            answers.append((i, answer))\n",
        "    return answers"
      ],
      "metadata": {
        "id": "dg9Ikaz5uNm-"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract an answer with additional context\n",
        "def get_answer_with_context(inputs, context_tokens=20):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        answer_start_scores, answer_end_scores = outputs.start_logits, outputs.end_logits\n",
        "\n",
        "    answer_start = torch.argmax(answer_start_scores)\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "    # Adding more context around the answer\n",
        "    start_context = max(answer_start - context_tokens, 0)\n",
        "    end_context = min(answer_end + context_tokens, inputs.input_ids.size(1))\n",
        "\n",
        "    answer_with_context = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs.input_ids[0][start_context:end_context]))\n",
        "    return answer_with_context"
      ],
      "metadata": {
        "id": "C7Z6zHMxuQCP"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is manta rays?\"\n",
        "answer = get_answer(tokenize_input(question, context), answer_strategy='expanded', expansion_tokens=100)\n",
        "display_answer(question, answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0owffIxuvX9",
        "outputId": "88256082-e644-41f6-b31b-f42f7ae2674b"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is manta rays?\n",
            "Answer: home to the largest population of giant manta ray , comprising over 22 , 000 individuals , with large aggregation sites within the waters of the machalilla national park and the galapagos marine reserve . overall , given their life history traits , particularly their low reproductive output , giant manta ray populations are inherently vulnerable to depletions , with low likelihood of recovery . additional research is needed to better understand the population structure and global distribution of the giant manta ray . manta rays are recognized by their large diamond - shaped body with elongated wing - like pectoral fins , ventrally - placed gill slits , laterally - placed eyes , and wide , terminal mouths . in front of the mouth , they have two structures called cephalic lobes which extend and help to channel water into the mouth for feeding activities ( making them the only vertebrate animals with three paired appendages ) . manta rays come in two distinct color types : chevron ( mostly black back and white belly ) and black ( almost completely black\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is manta rays?\"\n",
        "answer = get_answer(tokenize_input(question, context), answer_strategy='multiple', num_answers=3)\n",
        "display_answer(question, answer[0])\n",
        "display_answer(question, answer[1])\n",
        "display_answer(question, answer[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV4CsBQFu_qQ",
        "outputId": "2b06a0ae-38c5-49f2-c4fd-e794f927e43b"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is manta rays?\n",
            "Answer: (0, 'large diamond - shaped body')\n",
            "Question: What is manta rays?\n",
            "Answer: (1, 'world â€™ s largest ray with a wingspan of up to 26 feet . they are filter feeders and eat large quantities of zooplankton . giant manta rays are slow - growing , migratory animals with small , highly fragmented populations that are sparsely distributed across the world . the main threat to the giant manta ray is commercial fishing , with the species both targeted and caught as bycatch in a number of global fisheries throughout its range . manta rays are particularly valued for their gill plates , which are traded internationally . in 2018 , noaa fisheries listed the species as threatened under the endangered species act . the global population size is unknown . with the exception of ecuador , the few regional population estimates appear to be small , ranging from around 600 to 2 , 000 individuals , and in areas subject to fishing , have significantly declined . ecuador , on the other hand , is thought to be home to the largest population of giant manta ray , comprising over 22 , 000 individuals , with large aggregation sites within the waters of the machalilla national park and the galapagos marine reserve . overall , given their life history traits , particularly their low reproductive output , giant manta ray populations are inherently vulnerable to depletions , with low likelihood of recovery . additional research is needed to better understand the population structure and global distribution of the giant manta ray . manta rays are recognized by their large diamond - shaped body with elongated wing - like pectoral fins , ventrally - placed gill slits , laterally - placed eyes , and wide , terminal mouths')\n",
            "Question: What is manta rays?\n",
            "Answer: (2, 'their large diamond - shaped body with elongated wing - like pectoral fins')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is manta rays?\"\n",
        "answer = get_answer(tokenize_input(question, context), answer_strategy='with_context')\n",
        "display_answer(question, answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMSNQeVevY_e",
        "outputId": "7ab3a801-d180-43f7-a5b6-8244520c66ab"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is manta rays?\n",
            "Answer: the population structure and global distribution of the giant manta ray . manta rays are recognized by their large diamond - shaped body with elongated wing - like pectoral fins , ventrally - placed gill slits , laterally -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NWbms8OhvxJx"
      },
      "execution_count": 118,
      "outputs": []
    }
  ]
}
